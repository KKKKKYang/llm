{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.50.3\n",
      "Datasets library loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 1. 环境导入与依赖检查\n",
    "# 本 cell 导入必要的库，并打印各库版本，确保环境配置正确。\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Limit to 50% of total GPU memory for the current process\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5, device=0)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "from datasets import load_dataset\n",
    "print(\"Datasets library loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from dataset/jssp_3m3j.json\n",
      "Total examples: 2\n",
      "First example:\n",
      "{'input': 'JSSP Problem:\\nOptimize schedule for 3 Jobs across 3 Machines to minimize makespan.\\nJob 0: Operation 0 on Machine 0 for 105 minutes, Operation 1 on Machine 1 for 29 minutes, Operation 2 on Machine 2 for 213 minutes.\\nJob 1: Operation 0 on Machine 2 for 193 minutes, Operation 1 on Machine 1 for 18 minutes, Operation 2 on Machine 0 for 213 minutes.\\nJob 2: Operation 0 on Machine 0 for 78 minutes, Operation 1 on Machine 2 for 74 minutes, Operation 2 on Machine 1 for 221 minutes.\\nSolution:', 'output': 'Step-by-step solution:\\n1. Identify operation sequences:\\n   - Job 0: M0 (105) -> M1 (29) -> M2 (213)\\n   - Job 1: M2 (193) -> M1 (18) -> M0 (213)\\n   - Job 2: M0 (78) -> M2 (74) -> M1 (221)\\n2. At time 0, schedule the first operations:\\n   - Job 0 on M0 from 0 to 105.\\n   - Job 1 on M2 from 0 to 193.\\n   - Job 2 must wait for M0; start at 105 and finish at 183.\\n3. Schedule subsequent operations based on machine availability:\\n   - Job 0: Operation on M1 starts at 105 (finishes at 134), then on M2 starts at 193 (finishes at 406).\\n   - Job 1: Operation on M1 starts at 193 (finishes at 211), then on M0 starts at 211 (finishes at 424).\\n   - Job 2: Operation on M2 starts at 211 (finishes at 285), then on M1 starts at 424 (finishes at 645).\\n4. Final completion times: Job 0 at 406, Job 1 at 424, Job 2 at 645.\\nFinal makespan: 645 minutes.'}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 2. 数据加载：从 data 文件夹读取 jssp_3m3j.json 文件\n",
    "# 请确保在当前工作目录下存在 data/jssp_3m3j.json 文件，该文件采用 JSON 列表格式，\n",
    "# 每个样例包含 \"input\" 和 \"output\" 字段。\n",
    "\n",
    "# %%\n",
    "data_file = \"dataset/jssp_3m3j.json\"\n",
    "dataset = load_dataset(\"json\", data_files=data_file)\n",
    "print(\"Dataset loaded from\", data_file)\n",
    "print(\"Total examples:\", len(dataset[\"train\"]))\n",
    "print(\"First example:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1\n",
      "Evaluation set size: 1\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 3. 划分训练集和测试集\n",
    "# 为了观察训练进度和评估效果，我们将加载的数据集划分为训练集和测试集，\n",
    "# 这里使用 80% 的数据用于训练，20% 用于评估。\n",
    "\n",
    "# %%\n",
    "# 使用 train_test_split 划分数据集\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "print(\"Evaluation set size:\", len(eval_dataset))#:3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO CHOOSE RUN ON GPU OR CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer: microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\benar\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files: 100%|██████████| 2/2 [02:41<00:00, 80.87s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 4. 加载模型和 Tokenizer\n",
    "# 加载 \"microsoft/Phi-3.5-mini-instruct\" 模型及其对应的 tokenizer，\n",
    "# 并将模型设置为训练模式，同时打印加载状态。\n",
    "\n",
    "# %%\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "print(\"Loading model and tokenizer:\", model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              torch_dtype=torch.float16,\n",
    "\n",
    "#                                              device_map=\"auto\",  # 自动选择设备\n",
    "#                                             )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             torch_dtype=torch.float16,\n",
    "\n",
    "                                             device_map=\"cpu\",  # TRY THIS IF GPU NO MEM\n",
    "                                            )\n",
    "\n",
    "\n",
    "model.train()  # 训练模式\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 74.01 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 117.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Example from training set:\n",
      "{'input': 'JSSP Problem:\\nOptimize schedule for 3 Jobs across 3 Machines to minimize makespan.\\nJob 0: Operation 0 on Machine 0 for 105 minutes, Operation 1 on Machine 1 for 29 minutes, Operation 2 on Machine 2 for 213 minutes.\\nJob 1: Operation 0 on Machine 2 for 193 minutes, Operation 1 on Machine 1 for 18 minutes, Operation 2 on Machine 0 for 213 minutes.\\nJob 2: Operation 0 on Machine 0 for 78 minutes, Operation 1 on Machine 2 for 74 minutes, Operation 2 on Machine 1 for 221 minutes.\\nSolution:', 'output': 'Step-by-step solution:\\n1. Identify operation sequences:\\n   - Job 0: M0 (105) -> M1 (29) -> M2 (213)\\n   - Job 1: M2 (193) -> M1 (18) -> M0 (213)\\n   - Job 2: M0 (78) -> M2 (74) -> M1 (221)\\n2. At time 0, schedule the first operations:\\n   - Job 0 on M0 from 0 to 105.\\n   - Job 1 on M2 from 0 to 193.\\n   - Job 2 must wait for M0; start at 105 and finish at 183.\\n3. Schedule subsequent operations based on machine availability:\\n   - Job 0: Operation on M1 starts at 105 (finishes at 134), then on M2 starts at 193 (finishes at 406).\\n   - Job 1: Operation on M1 starts at 193 (finishes at 211), then on M0 starts at 211 (finishes at 424).\\n   - Job 2: Operation on M2 starts at 211 (finishes at 285), then on M1 starts at 424 (finishes at 645).\\n4. Final completion times: Job 0 at 406, Job 1 at 424, Job 2 at 645.\\nFinal makespan: 645 minutes.', 'input_ids': [435, 1799, 29925, 11583, 29901, 13, 20624, 326, 675, 20410, 363, 29871, 29941, 17163, 29879, 4822, 29871, 29941, 17197, 1475, 304, 6260, 675, 3732, 8357, 29889, 13, 11947, 29871, 29900, 29901, 20462, 29871, 29900, 373, 6189, 29871, 29900, 363, 29871, 29896, 29900, 29945, 6233, 29892, 20462, 29871, 29896, 373, 6189, 29871, 29896, 363, 29871, 29906, 29929, 6233, 29892, 20462, 29871, 29906, 373, 6189, 29871, 29906, 363, 29871, 29906, 29896, 29941, 6233, 29889, 13, 11947, 29871, 29896, 29901, 20462, 29871, 29900, 373, 6189, 29871, 29906, 363, 29871, 29896, 29929, 29941, 6233, 29892, 20462, 29871, 29896, 373, 6189, 29871, 29896, 363, 29871, 29896, 29947, 6233, 29892, 20462, 29871, 29906, 373, 6189, 29871, 29900, 363, 29871, 29906, 29896, 29941, 6233, 29889, 13, 11947, 29871, 29906, 29901, 20462, 29871, 29900, 373, 6189, 29871, 29900, 363, 29871, 29955, 29947, 6233, 29892, 20462, 29871, 29896, 373, 6189, 29871, 29906, 363, 29871, 29955, 29946, 6233, 29892, 20462, 29871, 29906, 373, 6189, 29871, 29896, 363, 29871, 29906, 29906, 29896, 6233, 29889, 13, 13296, 918, 29901, 14448, 29899, 1609, 29899, 10568, 1650, 29901, 13, 29896, 29889, 13355, 1598, 5858, 15602, 29901, 13, 259, 448, 17163, 29871, 29900, 29901, 341, 29900, 313, 29896, 29900, 29945, 29897, 1599, 341, 29896, 313, 29906, 29929, 29897, 1599, 341, 29906, 313, 29906, 29896, 29941, 29897, 13, 259, 448, 17163, 29871, 29896, 29901, 341, 29906, 313, 29896, 29929, 29941, 29897, 1599, 341, 29896, 313, 29896, 29947, 29897, 1599, 341, 29900, 313, 29906, 29896, 29941, 29897, 13, 259, 448, 17163, 29871, 29906, 29901, 341, 29900, 313, 29955, 29947, 29897, 1599, 341, 29906, 313, 29955, 29946, 29897, 1599, 341, 29896, 313, 29906, 29906, 29896, 29897, 13, 29906, 29889, 2180, 931, 29871, 29900, 29892, 20410, 278, 937, 6931, 29901, 13, 259, 448, 17163, 29871, 29900, 373, 341, 29900, 515, 29871, 29900, 304, 29871, 29896, 29900, 29945, 29889, 13, 259, 448, 17163, 29871, 29896, 373, 341, 29906, 515, 29871, 29900, 304, 29871, 29896, 29929, 29941, 29889, 13, 259, 448, 17163, 29871, 29906, 1818, 4480, 363, 341, 29900, 29936, 1369, 472, 29871, 29896, 29900, 29945, 322, 8341, 472, 29871, 29896, 29947, 29941, 29889, 13, 29941, 29889, 1102, 11272, 15352, 6931, 2729, 373, 4933, 20847, 3097, 29901, 13, 259, 448, 17163, 29871, 29900, 29901, 20462, 373, 341, 29896, 8665, 472, 29871, 29896, 29900, 29945, 313, 4951, 17006, 472, 29871, 29896, 29941, 29946, 511, 769, 373, 341, 29906, 8665, 472, 29871, 29896, 29929, 29941, 313, 4951, 17006, 472, 29871, 29946, 29900, 29953, 467, 13, 259, 448, 17163, 29871, 29896, 29901, 20462, 373, 341, 29896, 8665, 472, 29871, 29896, 29929, 29941, 313, 4951, 17006, 472, 29871, 29906, 29896, 29896, 511, 769, 373, 341, 29900, 8665, 472, 29871, 29906, 29896, 29896, 313, 4951, 17006, 472, 29871, 29946, 29906, 29946, 467, 13, 259, 448, 17163, 29871, 29906, 29901, 20462, 373, 341, 29906, 8665, 472, 29871, 29906, 29896, 29896, 313, 4951, 17006, 472, 29871, 29906, 29947, 29945, 511, 769, 373, 341, 29896, 8665, 472, 29871, 29946, 29906, 29946, 313, 4951, 17006, 472, 29871, 29953, 29946, 29945, 467, 13, 29946, 29889, 9550, 13285, 3064, 29901, 17163, 29871, 29900, 472, 29871, 29946, 29900, 29953, 29892, 17163, 29871, 29896, 472, 29871], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 5. 数据预处理：分词\n",
    "# 定义分词函数，将每个样例的 input 和 output 拼接为一个完整的文本后进行编码，\n",
    "# 设置最大长度为 512。分词结果将用于后续微调。\n",
    "\n",
    "# %%\n",
    "def tokenize_function(example):\n",
    "    # 拼接 input 和 output 字段\n",
    "    text = example[\"input\"] + example[\"output\"]\n",
    "    return tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "# 对训练集和评估集进行分词\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=False)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "print(\"Tokenization complete. Example from training set:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collator created successfully.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 6. 构造 Data Collator\n",
    "# 使用 DataCollatorForLanguageModeling 为因果语言模型构建数据整理器，\n",
    "# 注意设置 mlm=False，因为我们不使用掩码任务。\n",
    "\n",
    "# %%\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"Data Collator created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters:\n",
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./finetuned_model\\runs\\Apr04_01-04-17_Ben,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./finetuned_model,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Trainer initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 7. 配置训练参数与初始化 Trainer\n",
    "# 设置训练参数，如输出目录、每设备 batch size、训练轮数、日志打印频率、评估策略等，\n",
    "# 并初始化 Trainer。日志和评估配置将帮助你在训练过程中观察进度与性能。\n",
    "\n",
    "# %%\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    per_device_train_batch_size=1,      # 根据你的硬件配置调整 batch size\n",
    "    num_train_epochs=3,                   # 训练轮数，如有需要可调整\n",
    "    logging_steps=10,                     # 每 10 步打印一次日志\n",
    "    eval_steps=50,                        # 每 50 步进行一次评估\n",
    "    evaluation_strategy=\"steps\",          # 按步数进行评估\n",
    "    save_steps=100,                       # 每 100 步保存一次模型\n",
    "    fp16=True,                            # 如支持 fp16 则启用\n",
    "    save_total_limit=2,                   # 最多保存 2 个检查点\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Training parameters:\")\n",
    "print(training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 22:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Finetuned model saved to ./finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 8. Start Training and Monitor Progress\n",
    "# This cell starts the fine-tuning process. Training logs (including loss, etc.) will be output during the process.\n",
    "# After training, the fine-tuned model will be saved to the specified directory.\n",
    "\n",
    "# %%\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()  # Begin training; training logs will be printed automatically\n",
    "print(\"Training complete!\")\n",
    "trainer.save_model()  # Save the fine-tuned model\n",
    "print(\"Finetuned model saved to ./finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
