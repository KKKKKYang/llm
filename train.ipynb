{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.50.3\n",
      "Datasets library loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 1. ç¯å¢ƒå¯¼å…¥ä¸ä¾èµ–æ£€æŸ¥\n",
    "# æœ¬ cell å¯¼å…¥å¿…è¦çš„åº“ï¼Œå¹¶æ‰“å°å„åº“ç‰ˆæœ¬ï¼Œç¡®ä¿ç¯å¢ƒé…ç½®æ­£ç¡®ã€‚\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Limit to 50% of total GPU memory for the current process\n",
    "# torch.cuda.set_per_process_memory_fraction(0.5, device=0)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import transformers\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "from datasets import load_dataset\n",
    "print(\"Datasets library loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from dataset/jssp_3m3j.json\n",
      "Total examples: 2\n",
      "First example:\n",
      "{'input': 'JSSP Problem:\\nOptimize schedule for 3 Jobs across 3 Machines to minimize makespan.\\nJob 0: Operation 0 on Machine 0 for 105 minutes, Operation 1 on Machine 1 for 29 minutes, Operation 2 on Machine 2 for 213 minutes.\\nJob 1: Operation 0 on Machine 2 for 193 minutes, Operation 1 on Machine 1 for 18 minutes, Operation 2 on Machine 0 for 213 minutes.\\nJob 2: Operation 0 on Machine 0 for 78 minutes, Operation 1 on Machine 2 for 74 minutes, Operation 2 on Machine 1 for 221 minutes.\\nSolution:', 'output': 'Step-by-step solution:\\n1. Identify operation sequences:\\n   - Job 0: M0 (105) -> M1 (29) -> M2 (213)\\n   - Job 1: M2 (193) -> M1 (18) -> M0 (213)\\n   - Job 2: M0 (78) -> M2 (74) -> M1 (221)\\n2. At time 0, schedule the first operations:\\n   - Job 0 on M0 from 0 to 105.\\n   - Job 1 on M2 from 0 to 193.\\n   - Job 2 must wait for M0; start at 105 and finish at 183.\\n3. Schedule subsequent operations based on machine availability:\\n   - Job 0: Operation on M1 starts at 105 (finishes at 134), then on M2 starts at 193 (finishes at 406).\\n   - Job 1: Operation on M1 starts at 193 (finishes at 211), then on M0 starts at 211 (finishes at 424).\\n   - Job 2: Operation on M2 starts at 211 (finishes at 285), then on M1 starts at 424 (finishes at 645).\\n4. Final completion times: Job 0 at 406, Job 1 at 424, Job 2 at 645.\\nFinal makespan: 645 minutes.'}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 2. æ•°æ®åŠ è½½ï¼šä» data æ–‡ä»¶å¤¹è¯»å– jssp_3m3j.json æ–‡ä»¶\n",
    "# è¯·ç¡®ä¿åœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹å­˜åœ¨ data/jssp_3m3j.json æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶é‡‡ç”¨ JSON åˆ—è¡¨æ ¼å¼ï¼Œ\n",
    "# æ¯ä¸ªæ ·ä¾‹åŒ…å« \"input\" å’Œ \"output\" å­—æ®µã€‚\n",
    "\n",
    "# %%\n",
    "data_file = \"dataset/jssp_3m3j.json\"\n",
    "dataset = load_dataset(\"json\", data_files=data_file)\n",
    "print(\"Dataset loaded from\", data_file)\n",
    "print(\"Total examples:\", len(dataset[\"train\"]))\n",
    "print(\"First example:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1\n",
      "Evaluation set size: 1\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "# ä¸ºäº†è§‚å¯Ÿè®­ç»ƒè¿›åº¦å’Œè¯„ä¼°æ•ˆæœï¼Œæˆ‘ä»¬å°†åŠ è½½çš„æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œ\n",
    "# è¿™é‡Œä½¿ç”¨ 80% çš„æ•°æ®ç”¨äºè®­ç»ƒï¼Œ20% ç”¨äºè¯„ä¼°ã€‚\n",
    "\n",
    "# %%\n",
    "# ä½¿ç”¨ train_test_split åˆ’åˆ†æ•°æ®é›†\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "print(\"Evaluation set size:\", len(eval_dataset))#:3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO CHOOSE RUN ON GPU OR CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer: microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\benar\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [02:41<00:00, 80.87s/it] \n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 4. åŠ è½½æ¨¡å‹å’Œ Tokenizer\n",
    "# åŠ è½½ \"microsoft/Phi-3.5-mini-instruct\" æ¨¡å‹åŠå…¶å¯¹åº”çš„ tokenizerï¼Œ\n",
    "# å¹¶å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ŒåŒæ—¶æ‰“å°åŠ è½½çŠ¶æ€ã€‚\n",
    "\n",
    "# %%\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "print(\"Loading model and tokenizer:\", model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#                                              torch_dtype=torch.float16,\n",
    "\n",
    "#                                              device_map=\"auto\",  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡\n",
    "#                                             )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             torch_dtype=torch.float16,\n",
    "\n",
    "                                             device_map=\"cpu\",  # TRY THIS IF GPU NO MEM\n",
    "                                            )\n",
    "\n",
    "\n",
    "model.train()  # è®­ç»ƒæ¨¡å¼\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 74.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 117.06 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete. Example from training set:\n",
      "{'input': 'JSSP Problem:\\nOptimize schedule for 3 Jobs across 3 Machines to minimize makespan.\\nJob 0: Operation 0 on Machine 0 for 105 minutes, Operation 1 on Machine 1 for 29 minutes, Operation 2 on Machine 2 for 213 minutes.\\nJob 1: Operation 0 on Machine 2 for 193 minutes, Operation 1 on Machine 1 for 18 minutes, Operation 2 on Machine 0 for 213 minutes.\\nJob 2: Operation 0 on Machine 0 for 78 minutes, Operation 1 on Machine 2 for 74 minutes, Operation 2 on Machine 1 for 221 minutes.\\nSolution:', 'output': 'Step-by-step solution:\\n1. Identify operation sequences:\\n   - Job 0: M0 (105) -> M1 (29) -> M2 (213)\\n   - Job 1: M2 (193) -> M1 (18) -> M0 (213)\\n   - Job 2: M0 (78) -> M2 (74) -> M1 (221)\\n2. At time 0, schedule the first operations:\\n   - Job 0 on M0 from 0 to 105.\\n   - Job 1 on M2 from 0 to 193.\\n   - Job 2 must wait for M0; start at 105 and finish at 183.\\n3. Schedule subsequent operations based on machine availability:\\n   - Job 0: Operation on M1 starts at 105 (finishes at 134), then on M2 starts at 193 (finishes at 406).\\n   - Job 1: Operation on M1 starts at 193 (finishes at 211), then on M0 starts at 211 (finishes at 424).\\n   - Job 2: Operation on M2 starts at 211 (finishes at 285), then on M1 starts at 424 (finishes at 645).\\n4. Final completion times: Job 0 at 406, Job 1 at 424, Job 2 at 645.\\nFinal makespan: 645 minutes.', 'input_ids': [435, 1799, 29925, 11583, 29901, 13, 20624, 326, 675, 20410, 363, 29871, 29941, 17163, 29879, 4822, 29871, 29941, 17197, 1475, 304, 6260, 675, 3732, 8357, 29889, 13, 11947, 29871, 29900, 29901, 20462, 29871, 29900, 373, 6189, 29871, 29900, 363, 29871, 29896, 29900, 29945, 6233, 29892, 20462, 29871, 29896, 373, 6189, 29871, 29896, 363, 29871, 29906, 29929, 6233, 29892, 20462, 29871, 29906, 373, 6189, 29871, 29906, 363, 29871, 29906, 29896, 29941, 6233, 29889, 13, 11947, 29871, 29896, 29901, 20462, 29871, 29900, 373, 6189, 29871, 29906, 363, 29871, 29896, 29929, 29941, 6233, 29892, 20462, 29871, 29896, 373, 6189, 29871, 29896, 363, 29871, 29896, 29947, 6233, 29892, 20462, 29871, 29906, 373, 6189, 29871, 29900, 363, 29871, 29906, 29896, 29941, 6233, 29889, 13, 11947, 29871, 29906, 29901, 20462, 29871, 29900, 373, 6189, 29871, 29900, 363, 29871, 29955, 29947, 6233, 29892, 20462, 29871, 29896, 373, 6189, 29871, 29906, 363, 29871, 29955, 29946, 6233, 29892, 20462, 29871, 29906, 373, 6189, 29871, 29896, 363, 29871, 29906, 29906, 29896, 6233, 29889, 13, 13296, 918, 29901, 14448, 29899, 1609, 29899, 10568, 1650, 29901, 13, 29896, 29889, 13355, 1598, 5858, 15602, 29901, 13, 259, 448, 17163, 29871, 29900, 29901, 341, 29900, 313, 29896, 29900, 29945, 29897, 1599, 341, 29896, 313, 29906, 29929, 29897, 1599, 341, 29906, 313, 29906, 29896, 29941, 29897, 13, 259, 448, 17163, 29871, 29896, 29901, 341, 29906, 313, 29896, 29929, 29941, 29897, 1599, 341, 29896, 313, 29896, 29947, 29897, 1599, 341, 29900, 313, 29906, 29896, 29941, 29897, 13, 259, 448, 17163, 29871, 29906, 29901, 341, 29900, 313, 29955, 29947, 29897, 1599, 341, 29906, 313, 29955, 29946, 29897, 1599, 341, 29896, 313, 29906, 29906, 29896, 29897, 13, 29906, 29889, 2180, 931, 29871, 29900, 29892, 20410, 278, 937, 6931, 29901, 13, 259, 448, 17163, 29871, 29900, 373, 341, 29900, 515, 29871, 29900, 304, 29871, 29896, 29900, 29945, 29889, 13, 259, 448, 17163, 29871, 29896, 373, 341, 29906, 515, 29871, 29900, 304, 29871, 29896, 29929, 29941, 29889, 13, 259, 448, 17163, 29871, 29906, 1818, 4480, 363, 341, 29900, 29936, 1369, 472, 29871, 29896, 29900, 29945, 322, 8341, 472, 29871, 29896, 29947, 29941, 29889, 13, 29941, 29889, 1102, 11272, 15352, 6931, 2729, 373, 4933, 20847, 3097, 29901, 13, 259, 448, 17163, 29871, 29900, 29901, 20462, 373, 341, 29896, 8665, 472, 29871, 29896, 29900, 29945, 313, 4951, 17006, 472, 29871, 29896, 29941, 29946, 511, 769, 373, 341, 29906, 8665, 472, 29871, 29896, 29929, 29941, 313, 4951, 17006, 472, 29871, 29946, 29900, 29953, 467, 13, 259, 448, 17163, 29871, 29896, 29901, 20462, 373, 341, 29896, 8665, 472, 29871, 29896, 29929, 29941, 313, 4951, 17006, 472, 29871, 29906, 29896, 29896, 511, 769, 373, 341, 29900, 8665, 472, 29871, 29906, 29896, 29896, 313, 4951, 17006, 472, 29871, 29946, 29906, 29946, 467, 13, 259, 448, 17163, 29871, 29906, 29901, 20462, 373, 341, 29906, 8665, 472, 29871, 29906, 29896, 29896, 313, 4951, 17006, 472, 29871, 29906, 29947, 29945, 511, 769, 373, 341, 29896, 8665, 472, 29871, 29946, 29906, 29946, 313, 4951, 17006, 472, 29871, 29953, 29946, 29945, 467, 13, 29946, 29889, 9550, 13285, 3064, 29901, 17163, 29871, 29900, 472, 29871, 29946, 29900, 29953, 29892, 17163, 29871, 29896, 472, 29871], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 5. æ•°æ®é¢„å¤„ç†ï¼šåˆ†è¯\n",
    "# å®šä¹‰åˆ†è¯å‡½æ•°ï¼Œå°†æ¯ä¸ªæ ·ä¾‹çš„ input å’Œ output æ‹¼æ¥ä¸ºä¸€ä¸ªå®Œæ•´çš„æ–‡æœ¬åè¿›è¡Œç¼–ç ï¼Œ\n",
    "# è®¾ç½®æœ€å¤§é•¿åº¦ä¸º 512ã€‚åˆ†è¯ç»“æœå°†ç”¨äºåç»­å¾®è°ƒã€‚\n",
    "\n",
    "# %%\n",
    "def tokenize_function(example):\n",
    "    # æ‹¼æ¥ input å’Œ output å­—æ®µ\n",
    "    text = example[\"input\"] + example[\"output\"]\n",
    "    return tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†å’Œè¯„ä¼°é›†è¿›è¡Œåˆ†è¯\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=False)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "print(\"Tokenization complete. Example from training set:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collator created successfully.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 6. æ„é€  Data Collator\n",
    "# ä½¿ç”¨ DataCollatorForLanguageModeling ä¸ºå› æœè¯­è¨€æ¨¡å‹æ„å»ºæ•°æ®æ•´ç†å™¨ï¼Œ\n",
    "# æ³¨æ„è®¾ç½® mlm=Falseï¼Œå› ä¸ºæˆ‘ä»¬ä¸ä½¿ç”¨æ©ç ä»»åŠ¡ã€‚\n",
    "\n",
    "# %%\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"Data Collator created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters:\n",
      "TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=IntervalStrategy.STEPS,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./finetuned_model\\runs\\Apr04_01-04-17_Ben,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./finetuned_model,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Trainer initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 7. é…ç½®è®­ç»ƒå‚æ•°ä¸åˆå§‹åŒ– Trainer\n",
    "# è®¾ç½®è®­ç»ƒå‚æ•°ï¼Œå¦‚è¾“å‡ºç›®å½•ã€æ¯è®¾å¤‡ batch sizeã€è®­ç»ƒè½®æ•°ã€æ—¥å¿—æ‰“å°é¢‘ç‡ã€è¯„ä¼°ç­–ç•¥ç­‰ï¼Œ\n",
    "# å¹¶åˆå§‹åŒ– Trainerã€‚æ—¥å¿—å’Œè¯„ä¼°é…ç½®å°†å¸®åŠ©ä½ åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿè¿›åº¦ä¸æ€§èƒ½ã€‚\n",
    "\n",
    "# %%\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_model\",\n",
    "    per_device_train_batch_size=1,      # æ ¹æ®ä½ çš„ç¡¬ä»¶é…ç½®è°ƒæ•´ batch size\n",
    "    num_train_epochs=3,                   # è®­ç»ƒè½®æ•°ï¼Œå¦‚æœ‰éœ€è¦å¯è°ƒæ•´\n",
    "    logging_steps=10,                     # æ¯ 10 æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—\n",
    "    eval_steps=50,                        # æ¯ 50 æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°\n",
    "    evaluation_strategy=\"steps\",          # æŒ‰æ­¥æ•°è¿›è¡Œè¯„ä¼°\n",
    "    save_steps=100,                       # æ¯ 100 æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "    fp16=True,                            # å¦‚æ”¯æŒ fp16 åˆ™å¯ç”¨\n",
    "    save_total_limit=2,                   # æœ€å¤šä¿å­˜ 2 ä¸ªæ£€æŸ¥ç‚¹\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Training parameters:\")\n",
    "print(training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"Trainer initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 22:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Finetuned model saved to ./finetuned_model\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 8. Start Training and Monitor Progress\n",
    "# This cell starts the fine-tuning process. Training logs (including loss, etc.) will be output during the process.\n",
    "# After training, the fine-tuned model will be saved to the specified directory.\n",
    "\n",
    "# %%\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()  # Begin training; training logs will be printed automatically\n",
    "print(\"Training complete!\")\n",
    "trainer.save_model()  # Save the fine-tuned model\n",
    "print(\"Finetuned model saved to ./finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
